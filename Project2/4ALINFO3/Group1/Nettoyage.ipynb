{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24bbb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def valeur_plus_frequente_STR(liste):\n",
    "    # Filtrer les valeurs qui sont des chaînes de caractères et différentes de None\n",
    "    valeurs_str = [valeur for valeur in liste if (isinstance(valeur, str) or (isinstance(valeur, float) and not math.isnan(valeur))) and valeur is not None and valeur != \"nan\" and valeur != 0.0]\n",
    "    # Vérifier si la liste filtrée est vide\n",
    "    if not valeurs_str:\n",
    "        return None\n",
    "    # Utiliser Counter pour compter le nombre d'occurrences de chaque élément\n",
    "    compteur = Counter(valeurs_str)\n",
    "    # Utiliser max() avec une fonction lambda pour obtenir l'élément avec le plus grand nombre d'occurrences\n",
    "    valeur_plus_frequente = max(compteur, key=compteur.get)\n",
    "    return valeur_plus_frequente\n",
    "\n",
    "# Définir une fonction pour nettoyer chaque valeur de la colonne\n",
    "def clean_string(encoded_string):\n",
    "    if isinstance(encoded_string, str):  # Vérifiez si la valeur est une chaîne de caractères\n",
    "        # Décoder la chaîne depuis l'UTF-8 avec 'replace' pour gérer les caractères non décodables\n",
    "        decoded_string = encoded_string.encode('latin1').decode('utf-8', errors='replace')\n",
    "        # Réencoder la chaîne en latin1\n",
    "        cleaned_string = decoded_string.encode('latin1')\n",
    "    else:\n",
    "        return encoded_string  # Retourner la valeur telle quelle si elle n'est pas une chaîne de caractères\n",
    "\n",
    "def detect_csv_separator(file_path):\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        dialect = csv.Sniffer().sniff(csvfile.read(1024))  # Reads the first 1024 bytes to detect the delimiter\n",
    "        return dialect.delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b6e54218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Downloads\\caracteristiques-2017.csv\n",
      ",\n",
      "../Downloads\\caracteristiques-2018.csv\n",
      ",\n",
      "../Downloads\\caracteristiques-2019.csv\n",
      ";\n",
      "../Downloads\\caracteristiques-2020.csv\n",
      ";\n",
      "../Downloads\\carcteristiques-2021.csv\n",
      ";\n",
      "../Downloads\\carcteristiques-2022.csv\n",
      ";\n",
      "Accident_Id     int64\n",
      "jour            int64\n",
      "mois            int64\n",
      "an              int64\n",
      "hrmn           object\n",
      "lum             int64\n",
      "dep            object\n",
      "com            object\n",
      "agg             int64\n",
      "int             int64\n",
      "atm             int64\n",
      "col             int64\n",
      "adr            object\n",
      "lat            object\n",
      "long           object\n",
      "dtype: object\n",
      "Index(['float_columns Num_Acc', 'float_columns atm', 'float_columns col'], dtype='object')\n",
      "Index(['string_columns hrmn', 'string_columns adr', 'string_columns lat',\n",
      "       'string_columns long'],\n",
      "      dtype='object')\n",
      "valeur_plus_frequente adr AUTOROUTE A86\n",
      "valeur_plus_frequente lat 4800000.0\n",
      "valeur_plus_frequente long 228547.0\n",
      "end caracteristiques\n"
     ]
    }
   ],
   "source": [
    "#caracteristique\n",
    "# Get a list of all CSV files in the directory\n",
    "\n",
    "files = glob.glob('../Downloads/car*.csv')\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "# Loop through each file and append its data to the combined DataFrame\n",
    "for file in files:\n",
    "    print(file)\n",
    "    # Read the first row of the file to infer data types\n",
    "    separator = detect_csv_separator(file)\n",
    "    #dtypes = pd.read_csv(file, nrows=1,encoding='ansi').dtypes.to_dict()\n",
    "    print(separator)\n",
    "    # Read the entire CSV file using inferred data types\n",
    "    df = pd.read_csv(file, encoding='ansi', sep=separator, quotechar='\"')\n",
    "\n",
    "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    \n",
    "# Now 'combined_data' contains data from all CSV files\n",
    "combined_data['Num_Acc'] = combined_data['Num_Acc'].fillna(combined_data['Accident_Id'])\n",
    "combined_data = combined_data.drop(columns=['Accident_Id'])\n",
    "\n",
    "# Supprimer les colonnes \"gps\" et \"dep\" et \"com\"\n",
    "combined_data.drop(columns=['gps', 'dep', 'com'], inplace=True)\n",
    "column_types = df.dtypes\n",
    "print(column_types)\n",
    "float_columns = combined_data.select_dtypes(include=['float']).columns\n",
    "print('float_columns '+float_columns)\n",
    "combined_data[float_columns] = combined_data[float_columns].fillna(0)\n",
    "combined_data[float_columns] = combined_data[float_columns].astype(np.int64)\n",
    "\n",
    "string_columns = combined_data.select_dtypes(include=['object']).columns\n",
    "print('string_columns '+string_columns)\n",
    "\n",
    "adr_freq = str(valeur_plus_frequente_STR(combined_data['adr']))\n",
    "print('valeur_plus_frequente adr '+ adr_freq)\n",
    "combined_data['adr'].fillna(value=adr_freq, inplace=True)\n",
    "\n",
    "lat_freq = str(valeur_plus_frequente_STR(combined_data['lat']))\n",
    "print('valeur_plus_frequente lat '+ lat_freq)\n",
    "combined_data['lat'].fillna(value=lat_freq, inplace=True)\n",
    "\n",
    "long_freq = str(valeur_plus_frequente_STR(combined_data['long']))\n",
    "print('valeur_plus_frequente long '+ long_freq)\n",
    "combined_data['long'].fillna(value=long_freq, inplace=True)\n",
    "\n",
    "combined_data.to_csv('../csv/caracteristiques.csv', sep='|', index=False)\n",
    "print('end caracteristiques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64a82e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Downloads\\usagers-2017.csv\n",
      ",\n",
      "../Downloads\\usagers-2018.csv\n",
      ",\n",
      "../Downloads\\usagers-2019.csv\n",
      ";\n",
      "../Downloads\\usagers-2020.csv\n",
      ";\n",
      "../Downloads\\usagers-2021.csv\n",
      ";\n",
      "../Downloads\\usagers-2022.csv\n",
      ";\n",
      "Index(['float_columns place', 'float_columns trajet', 'float_columns secu',\n",
      "       'float_columns locp', 'float_columns etatp', 'float_columns an_nais',\n",
      "       'float_columns secu1', 'float_columns secu2', 'float_columns secu3'],\n",
      "      dtype='object')\n",
      "Index(['string_columns actp', 'string_columns num_veh',\n",
      "       'string_columns id_vehicule', 'string_columns id_usager'],\n",
      "      dtype='object')\n",
      "end Usager\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#usagers\n",
    "# Get a list of all CSV files in the directory\n",
    "files = glob.glob('../Downloads/usagers*.csv')\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and append its data to the combined DataFrame\n",
    "for file in files:\n",
    "    print(file)\n",
    "    # Read the first row of the file to infer data types\n",
    "    separator = detect_csv_separator(file)\n",
    "    #dtypes = pd.read_csv(file, nrows=1).dtypes.to_dict()\n",
    "    print(separator)\n",
    "    # Read the entire CSV file using inferred data types\n",
    "    df = pd.read_csv(file, encoding='ansi', sep=separator, quotechar='\"')\n",
    "\n",
    "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    \n",
    "# Now 'combined_data' contains data from all CSV files\n",
    "\n",
    "float_columns = combined_data.select_dtypes(include=['float']).columns\n",
    "print('float_columns '+float_columns)\n",
    "combined_data[float_columns] = combined_data[float_columns].fillna(0)\n",
    "combined_data[float_columns] = combined_data[float_columns].astype(np.int64)\n",
    "combined_data['actp']\n",
    "string_columns = combined_data.select_dtypes(include=['object']).columns\n",
    "print('string_columns '+string_columns)\n",
    "\n",
    "combined_data[string_columns] = combined_data[string_columns].fillna(value='na')\n",
    "\n",
    "combined_data.to_csv('../csv/usagers.csv', sep='|', index=False)\n",
    "\n",
    "print('end Usager')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c849c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Downloads\\vehicules-2017.csv\n",
      ",\n",
      "../Downloads\\vehicules-2018.csv\n",
      ",\n",
      "../Downloads\\vehicules-2019.csv\n",
      ";\n",
      "../Downloads\\vehicules-2020.csv\n",
      ";\n",
      "../Downloads\\vehicules-2021.csv\n",
      ";\n",
      "../Downloads\\vehicules-2022.csv\n",
      ";\n",
      "end vehicules\n"
     ]
    }
   ],
   "source": [
    "#vehicules\n",
    "# Get a list of all CSV files in the directory\n",
    "files = glob.glob('../Downloads/vehicules*.csv')\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and append its data to the combined DataFrame\n",
    "for file in files:\n",
    "    print(file)\n",
    "    # Read the first row of the file to infer data types\n",
    "    separator = detect_csv_separator(file)\n",
    "    #dtypes = pd.read_csv(file, nrows=1).dtypes.to_dict()\n",
    "    print(separator)\n",
    "    # Read the entire CSV file using inferred data types\n",
    "    df = pd.read_csv(file, encoding='ansi', sep=separator, quotechar='\"')\n",
    "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "\n",
    "# Now 'combined_data' contains data from all CSV files\n",
    "combined_data['num_veh'] = combined_data['num_veh'].fillna(combined_data['id_vehicule'])\n",
    "combined_data = combined_data.drop(columns=['id_vehicule'])\n",
    "\n",
    "float_columns = combined_data.select_dtypes(include=['float']).columns\n",
    "combined_data[float_columns] = combined_data[float_columns].fillna(0)\n",
    "combined_data[float_columns] = combined_data[float_columns].astype(np.int64)\n",
    "string_columns = combined_data.select_dtypes(include=['object']).columns\n",
    "combined_data[string_columns] = combined_data[string_columns].fillna(value='na')\n",
    "\n",
    "combined_data.to_csv('../csv/vehicules.csv', sep='|',  index=False)\n",
    "print('end vehicules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "50a4600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Downloads\\lieux-2017.csv\n",
      ",\n",
      "../Downloads\\lieux-2018.csv\n",
      ",\n",
      "../Downloads\\lieux-2019.csv\n",
      ";\n",
      "../Downloads\\lieux-2020.csv\n",
      ";\n",
      "../Downloads\\lieux-2021.csv\n",
      ";\n",
      "../Downloads\\lieux-2022.csv\n",
      ";\n",
      "Index(['circ', 'vosp', 'prof', 'plan', 'surf', 'infra', 'situ', 'vma'], dtype='object')\n",
      "Index(['voie', 'lartpc', 'larrout'], dtype='object')\n",
      "valeur_plus_frequente voie 0\n",
      "valeur_plus_frequente lartpc 15.0\n",
      "valeur_plus_frequente larrout  -1\n",
      "end lieux\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#lieux\n",
    "# Get a list of all CSV files in the directory\n",
    "files = glob.glob('../Downloads/lieux*.csv')\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and append its data to the combined DataFrame\n",
    "for file in files:\n",
    "    print(file)\n",
    "    # Read the first row of the file to infer data types\n",
    "    separator = detect_csv_separator(file)\n",
    "    #dtypes = pd.read_csv(file, nrows=1).dtypes.to_dict()\n",
    "    print(separator)\n",
    "    # Read the entire CSV file using inferred data types\n",
    "    df = pd.read_csv(file, encoding='ansi', sep=separator, quotechar='\"',low_memory=False)\n",
    "    df['voie'] = df['voie'].astype(str)\n",
    "    # Fill missing values with a specific value, e.g., 'Unknown'\n",
    "    df['voie'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "\n",
    "# Now 'combined_data' contains data from all CSV files\n",
    "# Supprimer les colonnes \"v1\" et \"v2\" et \"env1\"\n",
    "combined_data.drop(columns=['v1', 'v2', 'env1', 'pr' , 'pr1', 'nbv'], inplace=True)\n",
    "\n",
    "float_columns = combined_data.select_dtypes(include=['float']).columns\n",
    "print(float_columns)\n",
    "combined_data[float_columns] = combined_data[float_columns].fillna(0)\n",
    "combined_data[float_columns] = combined_data[float_columns].astype(np.int64)\n",
    "string_columns = combined_data.select_dtypes(include=['object']).columns\n",
    "print(string_columns)\n",
    "\n",
    "voie_freq = str(valeur_plus_frequente_STR(combined_data['voie']))\n",
    "print('valeur_plus_frequente voie '+ voie_freq)\n",
    "combined_data['voie'].fillna(value=voie_freq, inplace=True)\n",
    "\n",
    "lartpc_freq = str(valeur_plus_frequente_STR(combined_data['lartpc']))\n",
    "print('valeur_plus_frequente lartpc '+ lartpc_freq)\n",
    "combined_data['lartpc'].fillna(value=lartpc_freq, inplace=True)\n",
    "\n",
    "larrout_freq = str(valeur_plus_frequente_STR(combined_data['larrout']))\n",
    "print('valeur_plus_frequente larrout '+ larrout_freq)\n",
    "combined_data['larrout'].fillna(value=larrout_freq, inplace=True)\n",
    "\n",
    "combined_data.to_csv('../csv/lieux.csv', sep='|',  index=False)\n",
    "print('end lieux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e346ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c64120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
