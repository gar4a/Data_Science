{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAGIeEZQOPph"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Hardcode the job position and location\n",
        "ville = 'international'\n",
        "\n",
        "# Initialize the list to store job data\n",
        "data = []\n",
        "\n",
        "# Hardcode the URL with specific values for job position and location\n",
        "url = f'https://www.bayt.com/en/international/jobs/'\n",
        "\n",
        "# Function to extract job data from a job listing\n",
        "def extract_job_data(job_listing):\n",
        "    # Extract job ID from the 'data-job-id' attribute\n",
        "    job_id = job_listing['data-job-id']\n",
        "    job_details_url = f'https://www.bayt.com/en/{ville}/jobs/?jobId={job_id}'\n",
        "    detailsResponse = requests.get(job_details_url)\n",
        "    # Parse the HTML content of the response using BeautifulSoup\n",
        "    soupDetails = BeautifulSoup(detailsResponse.content, 'html.parser')\n",
        "    # Extract the new job description from the specific element\n",
        "    job_description_element = soupDetails.find('div', class_='jb-descr m10t t-small').text.strip()\n",
        "    # Extract other relevant information\n",
        "    job_title = job_listing.find('h2', class_='jb-title').text.strip()\n",
        "    company = job_listing.find('b', class_='jb-company').text.strip()\n",
        "    location = job_listing.find('span', class_='jb-loc').text.strip()\n",
        "    job_description = job_description_element\n",
        "    date_posted = job_listing.find('div', class_='jb-date').text.strip()\n",
        "\n",
        "    # Store the extracted data in a dictionary\n",
        "    job_data = {\n",
        "        'Job ID': job_id,\n",
        "        'Job Title': job_title,\n",
        "        'Company': company,\n",
        "        'Location': location,\n",
        "        'Job Description': job_description,\n",
        "        'Date Posted': date_posted,\n",
        "        'Fraudulent': 0\n",
        "    }\n",
        "\n",
        "    return job_data\n",
        "\n",
        "# Loop to load multiple pages\n",
        "page = 1\n",
        "while True:\n",
        "    # Send a GET request to the URL and store the response\n",
        "    response = requests.get(url, params={'page': page})\n",
        "\n",
        "    # Parse the HTML content of the response using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Loop through each job listing on the page\n",
        "    for job_listing in soup.find_all('li', {'data-js-job': True}):\n",
        "        # Extract job data and append to the 'data' list\n",
        "        data.append(extract_job_data(job_listing))\n",
        "\n",
        "    # Check if there are more results by looking for the \"More Results\" button\n",
        "    more_results_button = soup.find('a', class_='jsAjaxLoad btn u-expanded')\n",
        "    if more_results_button:\n",
        "        # Increment the page number for the next request\n",
        "        page += 1\n",
        "    else:\n",
        "        # Break the loop if no more results are found\n",
        "        break\n",
        "\n",
        "# Convert the list of dictionaries to a Pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print or save the DataFrame as needed\n",
        "print(df)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('jobs_posts.csv', index=False, encoding='utf-8')\n",
        "\n"
      ]
    }
  ]
}